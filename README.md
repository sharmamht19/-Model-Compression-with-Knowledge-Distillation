# Model-Compression-with-Knowledge-Distillation
Two-stage Multi-teacher Knowledge Distillation (TMKD) for Web QA compresses complex models like BERT into efficient student models with minimal information loss. TMKD leverages multiple teachers to ensure high accuracy and significantly faster inference, outperforming baseline models and matching the original teachers' performance. 
